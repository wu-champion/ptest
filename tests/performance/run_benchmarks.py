#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§è§„æ¨¡æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬ / Large-Scale Performance Benchmark Script

æŒ‰ç…§ BENCHMARK_PLAN.md æ‰§è¡Œæ€§èƒ½æµ‹è¯•
"""

import json
import sys
import time
import tracemalloc
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


class BenchmarkReporter:
    """åŸºå‡†æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, output_dir: str = "benchmark_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results: Dict[str, List[Dict]] = {}

    def add_result(
        self,
        category: str,
        name: str,
        value: float,
        unit: str = "s",
        status: str = "pass",
        details: Optional[Dict] = None,
    ):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        if category not in self.results:
            self.results[category] = []

        result = {
            "name": name,
            "value": value,
            "unit": unit,
            "status": status,
            "timestamp": datetime.now().isoformat(),
        }
        if details:
            result["details"] = details

        self.results[category].append(result)

    def generate_report(self) -> Dict[str, Any]:
        """ç”ŸæˆæŠ¥å‘Š"""
        import platform

        return {
            "metadata": {
                "version": "1.1.0",
                "timestamp": datetime.now().isoformat(),
                "python_version": platform.python_version(),
                "platform": platform.platform(),
                "processor": platform.processor(),
            },
            "results": self.results,
            "summary": self._calculate_summary(),
        }

    def _calculate_summary(self) -> Dict[str, Any]:
        """è®¡ç®—æ‘˜è¦ç»Ÿè®¡"""
        summary = {}
        for category, tests in self.results.items():
            values = [t["value"] for t in tests if t.get("status") == "pass"]
            failed = len([t for t in tests if t.get("status") == "fail"])

            summary[category] = {
                "count": len(tests),
                "passed": len(tests) - failed,
                "failed": failed,
                "total": sum(values) if values else 0,
                "avg": sum(values) / len(values) if values else 0,
                "min": min(values) if values else 0,
                "max": max(values) if values else 0,
            }
        return summary

    def save_json(self, filename: Optional[str] = None) -> Path:
        """ä¿å­˜ JSON æŠ¥å‘Š"""
        if filename is None:
            filename = f"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        filepath = self.output_dir / filename
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(self.generate_report(), f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š JSON æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
        return filepath

    def save_markdown(self, filename: Optional[str] = None) -> Path:
        """ä¿å­˜ Markdown æŠ¥å‘Š"""
        if filename is None:
            filename = f"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        filepath = self.output_dir / filename
        report = self.generate_report()

        md = []
        md.append("# Performance Benchmark Report")
        md.append(f"\n**Version**: {report['metadata']['version']}")
        md.append(f"**Date**: {report['metadata']['timestamp']}")
        md.append(f"**Python**: {report['metadata']['python_version']}")
        md.append(f"**Platform**: {report['metadata']['platform']}")
        md.append("\n---\n")

        # æ‘˜è¦
        md.append("## ğŸ“Š Summary\n")
        for category, stats in report["summary"].items():
            md.append(f"### {category}")
            md.append(
                f"- Tests: {stats['count']} (âœ“ {stats['passed']}, âœ— {stats['failed']})"
            )
            md.append(f"- Total: {stats['total']:.4f}s")
            md.append(f"- Average: {stats['avg']:.4f}s")
            md.append(f"- Min: {stats['min']:.4f}s")
            md.append(f"- Max: {stats['max']:.4f}s")
            md.append("")

        # è¯¦ç»†ç»“æœ
        md.append("\n## ğŸ“ˆ Detailed Results\n")
        for category, tests in report["results"].items():
            md.append(f"### {category}\n")
            md.append("| Test | Value | Unit | Status |")
            md.append("|------|-------|------|--------|")
            for test in tests:
                status_icon = "âœ“" if test.get("status") == "pass" else "âœ—"
                md.append(
                    f"| {test['name']} | {test['value']:.4f} | {test['unit']} | {status_icon} |"
                )
            md.append("")

        with open(filepath, "w", encoding="utf-8") as f:
            f.write("\n".join(md))

        print(f"ğŸ“ Markdown æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
        return filepath


def measure_memory_and_time(func, *args, **kwargs) -> Tuple[Any, float, float]:
    """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´å’Œå†…å­˜ä½¿ç”¨"""
    tracemalloc.start()
    start_time = time.time()

    result = func(*args, **kwargs)

    elapsed = time.time() - start_time
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # è½¬æ¢ä¸º MB
    peak_mb = peak / 1024 / 1024

    return result, elapsed, peak_mb


def generate_large_data(
    generator, data_type: str, total_count: int, batch_size: int = 10000
):
    """åˆ†æ‰¹ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®ï¼ˆæ”¯æŒè¶…è¿‡å•æ¬¡ MAX_GENERATION_COUNT é™åˆ¶çš„æ•°æ®ç”Ÿæˆï¼‰"""
    import tracemalloc

    tracemalloc.start()
    start_time = time.time()

    total_generated = 0
    num_batches = (total_count + batch_size - 1) // batch_size

    for i in range(num_batches):
        current_batch_size = min(batch_size, total_count - total_generated)
        if current_batch_size <= 0:
            break
        batch = generator.generate(data_type, count=current_batch_size, format="raw")
        total_generated += len(batch)
        # ä¸ä¿å­˜ç»“æœï¼Œä»…æµ‹è¯•ç”Ÿæˆé€Ÿåº¦

    elapsed = time.time() - start_time
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    return total_generated, elapsed, peak / 1024 / 1024


def run_data_generation_benchmark(reporter: BenchmarkReporter):
    """æ•°æ®ç”Ÿæˆæ€§èƒ½æµ‹è¯• - å¤§è§„æ¨¡"""
    print("\nğŸ”§ Running Data Generation Benchmarks...")

    from ptest.data import DataGenerator

    generator = DataGenerator()

    # åŸºå‡†æµ‹è¯•: 1,000 æ¡
    print("  æµ‹è¯• 1,000 æ¡æ•°æ®ç”Ÿæˆ (åŸºå‡†)...")
    start = time.time()
    result = generator.generate("name", count=1000, format="raw")
    elapsed = time.time() - start
    reporter.add_result("Data Generation (1K)", "Generate 1K Names", elapsed)
    print(f"    âœ“ Generate 1,000 names: {elapsed:.4f}s ({len(result)} items)")

    # æ ‡å‡†æµ‹è¯•: 10,000 æ¡
    print("  æµ‹è¯• 10,000 æ¡æ•°æ®ç”Ÿæˆ (æ ‡å‡†)...")
    start = time.time()
    result = generator.generate("email", count=10000, format="raw")
    elapsed = time.time() - start
    status = "pass" if elapsed < 1.0 else "fail"
    reporter.add_result(
        "Data Generation (10K)",
        "Generate 10K Emails",
        elapsed,
        status=status,
        details={"target": "< 1.0s"},
    )
    print(
        f"    {'âœ“' if status == 'pass' else 'âœ—'} Generate 10,000 emails: {elapsed:.4f}s"
    )

    # æ ‡å‡†æµ‹è¯•: 100,000 æ¡
    print("  æµ‹è¯• 100,000 æ¡æ•°æ®ç”Ÿæˆ (æ ‡å‡†)...")
    start = time.time()
    result = generator.generate("name", count=100000, format="raw")
    elapsed = time.time() - start
    status = "pass" if elapsed < 3.0 else "fail"
    reporter.add_result(
        "Data Generation (100K)",
        "Generate 100K Names",
        elapsed,
        status=status,
        details={"target": "< 3.0s", "count": len(result)},
    )
    print(
        f"    {'âœ“' if status == 'pass' else 'âœ—'} Generate 100,000 names: {elapsed:.4f}s (target: < 3s)"
    )

    # å‹åŠ›æµ‹è¯•: 1,000,000 æ¡ (åˆ†æ‰¹ç”Ÿæˆ)
    print("  æµ‹è¯• 1,000,000 æ¡æ•°æ®ç”Ÿæˆ (å‹åŠ› - åˆ†æ‰¹å¤„ç†)...")
    total_generated, elapsed, peak_mb = generate_large_data(generator, "uuid", 1000000)
    status = "pass" if elapsed < 60.0 and peak_mb < 2000 else "fail"
    reporter.add_result(
        "Data Generation (1M)",
        "Generate 1M UUIDs (batched)",
        elapsed,
        status=status,
        details={
            "total_generated": total_generated,
            "memory_mb": peak_mb,
            "target": "< 60s",
        },
    )
    print(
        f"    {'âœ“' if status == 'pass' else 'âœ—'} Generate 1,000,000 UUIDs: {elapsed:.4f}s, Memory: {peak_mb:.2f}MB"
    )


def run_suite_management_benchmark(reporter: BenchmarkReporter):
    """å¥—ä»¶ç®¡ç†æ€§èƒ½æµ‹è¯• - å¤§è§„æ¨¡"""
    print("\nğŸ“¦ Running Suite Management Benchmarks...")

    from ptest.suites import SuiteManager, TestSuite, CaseRef

    import tempfile

    with tempfile.TemporaryDirectory() as tmpdir:
        manager = SuiteManager(storage_dir=tmpdir)

        # æµ‹è¯• 1: 100 ä¸ªç”¨ä¾‹
        print("  æµ‹è¯• 100 ä¸ªç”¨ä¾‹å¥—ä»¶...")
        suite_data = {
            "name": "suite_100",
            "cases": [{"case_id": f"case_{i}", "order": i} for i in range(100)],
        }

        start = time.time()
        suite = manager.create_suite(suite_data)
        elapsed = time.time() - start
        reporter.add_result(
            "Suite Management (100)", "Create Suite (100 cases)", elapsed
        )
        print(f"    âœ“ Create suite with 100 cases: {elapsed:.4f}s")

        # æµ‹è¯• 2: 1,000 ä¸ªç”¨ä¾‹
        print("  æµ‹è¯• 1,000 ä¸ªç”¨ä¾‹å¥—ä»¶...")
        suite_data = {
            "name": "suite_1k",
            "cases": [{"case_id": f"case_{i}", "order": i} for i in range(1000)],
        }

        start = time.time()
        suite = manager.create_suite(suite_data)
        elapsed = time.time() - start
        status = "pass" if elapsed < 1.0 else "fail"
        reporter.add_result(
            "Suite Management (1K)",
            "Create Suite (1K cases)",
            elapsed,
            status=status,
            details={"target": "< 1.0s"},
        )
        print(
            f"    {'âœ“' if status == 'pass' else 'âœ—'} Create suite with 1,000 cases: {elapsed:.4f}s"
        )

        # æµ‹è¯• 3: 1,000 ä¸ªç”¨ä¾‹å¸¦ä¾èµ–
        print("  æµ‹è¯• 1,000 ä¸ªç”¨ä¾‹å¸¦ä¾èµ– (é“¾å¼ä¾èµ–)...")
        cases = []
        for i in range(1000):
            case = {"case_id": f"case_dep_{i}", "order": i}
            if i > 0 and i % 10 == 0:  # æ¯ 10 ä¸ªä¸€ç»„ä¾èµ–
                case["depends_on"] = [f"case_dep_{i - 1}"]
            cases.append(case)

        suite_data = {"name": "suite_1k_dep", "cases": cases}

        start = time.time()
        suite = manager.create_suite(suite_data)
        elapsed = time.time() - start
        status = "pass" if elapsed < 2.0 else "fail"
        reporter.add_result(
            "Suite Management (1K with deps)",
            "Create Suite (1K with dependencies)",
            elapsed,
            status=status,
            details={"target": "< 2.0s"},
        )
        print(
            f"    {'âœ“' if status == 'pass' else 'âœ—'} Create suite with 1,000 cases (with deps): {elapsed:.4f}s"
        )

        # æµ‹è¯• 4: æ‹“æ‰‘æ’åºæ€§èƒ½
        print("  æµ‹è¯•æ‹“æ‰‘æ’åºæ€§èƒ½ (1,000 ä¸ªç”¨ä¾‹)...")
        suite = TestSuite(
            name="topo_test",
            cases=[CaseRef(case_id=f"case_{i}", order=i) for i in range(1000)],
        )

        start = time.time()
        for _ in range(100):  # æ‰§è¡Œ 100 æ¬¡å–å¹³å‡
            suite.validate()
        elapsed = (time.time() - start) / 100
        status = "pass" if elapsed < 0.01 else "fail"
        reporter.add_result(
            "Suite Management (Topology)",
            "Validate Suite (1K cases avg)",
            elapsed,
            status=status,
            details={"target": "< 0.01s", "iterations": 100},
        )
        print(
            f"    {'âœ“' if status == 'pass' else 'âœ—'} Validate suite (avg of 100): {elapsed:.4f}s"
        )

        # å®é™…å·¥ç¨‹åœºæ™¯æµ‹è¯•
        print("\n  ğŸ“‚ å®é™…å·¥ç¨‹åœºæ™¯æµ‹è¯•...")

        # TEST-SM-005: é«˜å¯†åº¦ç”¨ä¾‹è„šæœ¬ï¼ˆ100 Cases/æ–‡ä»¶ï¼‰
        print("    TEST-SM-005: é«˜å¯†åº¦ç”¨ä¾‹è„šæœ¬ (100 Cases/æ–‡ä»¶)...")
        suite_data = {
            "name": "high_density_100",
            "cases": [
                {"case_id": f"test_method_{i:03d}", "order": i} for i in range(100)
            ],
        }
        start = time.time()
        suite = manager.create_suite(suite_data)
        elapsed = time.time() - start
        status = "pass" if elapsed < 0.5 else "fail"
        reporter.add_result(
            "Suite Management (Real-world)",
            "High Density Script (100 Cases/file)",
            elapsed,
            status=status,
            details={
                "target": "< 0.5s",
                "scenario": "Python file with 100 test methods",
            },
        )
        print(
            f"      {'âœ“' if status == 'pass' else 'âœ—'} 100 cases script: {elapsed:.4f}s"
        )

        # TEST-SM-006: å¤§å‹é›†æˆæµ‹è¯•è„šæœ¬ï¼ˆ500 Cases/æ–‡ä»¶ï¼‰
        print("    TEST-SM-006: å¤§å‹é›†æˆæµ‹è¯•è„šæœ¬ (500 Cases/æ–‡ä»¶)...")
        suite_data = {
            "name": "integration_test_500",
            "cases": [
                {"case_id": f"integration_step_{i:03d}", "order": i} for i in range(500)
            ],
        }
        start = time.time()
        suite = manager.create_suite(suite_data)
        elapsed = time.time() - start
        status = "pass" if elapsed < 2.0 else "fail"
        reporter.add_result(
            "Suite Management (Real-world)",
            "Integration Test Script (500 Cases/file)",
            elapsed,
            status=status,
            details={"target": "< 2.0s", "scenario": "Complex business flow test"},
        )
        print(
            f"      {'âœ“' if status == 'pass' else 'âœ—'} 500 cases script: {elapsed:.4f}s"
        )

        # TEST-SM-007: SQLæ‰¹é‡éªŒè¯è„šæœ¬ï¼ˆ1000 SQL/æ–‡ä»¶ï¼‰
        print("    TEST-SM-007: SQLæ‰¹é‡éªŒè¯è„šæœ¬ (1000 SQL/æ–‡ä»¶)...")
        suite_data = {
            "name": "sql_batch_1000",
            "cases": [
                {"case_id": f"sql_validation_{i:04d}", "order": i} for i in range(1000)
            ],
        }
        start = time.time()
        suite = manager.create_suite(suite_data)
        elapsed = time.time() - start
        status = "pass" if elapsed < 3.0 else "fail"
        reporter.add_result(
            "Suite Management (Real-world)",
            "SQL Batch Script (1000 SQL/file)",
            elapsed,
            status=status,
            details={"target": "< 3.0s", "scenario": "Database validation script"},
        )
        print(
            f"      {'âœ“' if status == 'pass' else 'âœ—'} 1000 SQL script: {elapsed:.4f}s"
        )

        # TEST-SM-008: ç‰ˆæœ¬è¿­ä»£ç”¨ä¾‹å†—ä½™æ¨¡æ‹Ÿ
        print("    TEST-SM-008: ç‰ˆæœ¬è¿­ä»£ç”¨ä¾‹å†—ä½™æ¨¡æ‹Ÿ...")
        cases = []
        # v1.0: åŠŸèƒ½Aï¼ˆ100 Casesï¼‰
        for i in range(100):
            cases.append({"case_id": f"v1_feature_A_{i:03d}", "order": len(cases)})
        # v2.0: åŠŸèƒ½Bä¾èµ–Aï¼ˆ100 Casesï¼Œå…¶ä¸­30ä¸ªé‡å¤æµ‹è¯•Aï¼‰
        for i in range(100):
            case = {"case_id": f"v2_feature_B_{i:03d}", "order": len(cases)}
            if i < 30:  # 30ä¸ªé‡å¤æµ‹è¯•A
                case["depends_on"] = [f"v1_feature_A_{i:03d}"]
            cases.append(case)
        # v3.0: åŠŸèƒ½Cä¾èµ–A+Bï¼ˆ100 Casesï¼Œå…¶ä¸­50ä¸ªé‡å¤æµ‹è¯•ï¼‰
        for i in range(100):
            case = {"case_id": f"v3_feature_C_{i:03d}", "order": len(cases)}
            if i < 25:  # 25ä¸ªé‡å¤æµ‹è¯•A
                case["depends_on"] = [f"v1_feature_A_{i:03d}"]
            elif i < 50:  # 25ä¸ªé‡å¤æµ‹è¯•B
                case["depends_on"] = [f"v2_feature_B_{i - 25:03d}"]
            cases.append(case)

        suite_data = {"name": "version_iteration_redundancy", "cases": cases}
        start = time.time()
        suite = manager.create_suite(suite_data)
        elapsed = time.time() - start
        status = "pass" if elapsed < 3.0 else "fail"
        reporter.add_result(
            "Suite Management (Real-world)",
            "Version Iteration Redundancy (300 Cases)",
            elapsed,
            status=status,
            details={
                "target": "< 3.0s",
                "scenario": "Multi-version with redundant cases",
                "total_cases": 300,
                "effective_coverage": "~200",
            },
        )
        print(
            f"      {'âœ“' if status == 'pass' else 'âœ—'} Version iteration (300 cases): {elapsed:.4f}s"
        )


def run_parallel_execution_benchmark(reporter: BenchmarkReporter):
    """å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯• - å¤§è§„æ¨¡"""
    print("\nâš¡ Running Parallel Execution Benchmarks...")

    from ptest.execution import ExecutionTask, ParallelExecutor, SequentialExecutor

    def task_func(duration: float = 0.01) -> str:
        """æ¨¡æ‹Ÿæµ‹è¯•ç”¨ä¾‹æ‰§è¡Œ"""
        time.sleep(duration)
        return "done"

    # æµ‹è¯• 1: 10 ä¸ªä»»åŠ¡ä¸²è¡Œ
    print("  æµ‹è¯• 10 ä¸ªä»»åŠ¡ä¸²è¡Œ...")
    tasks = [
        ExecutionTask(task_id=f"task_{i}", func=lambda: task_func(0.01))
        for i in range(10)
    ]

    sequential = SequentialExecutor()
    start = time.time()
    sequential.execute(tasks)
    seq_time_10 = time.time() - start
    reporter.add_result("Execution (10)", "Sequential (10 tasks)", seq_time_10)
    print(f"    âœ“ Sequential (10 tasks): {seq_time_10:.4f}s")

    # æµ‹è¯• 2: 10 ä¸ªä»»åŠ¡å¹¶è¡Œ
    print("  æµ‹è¯• 10 ä¸ªä»»åŠ¡å¹¶è¡Œ...")
    tasks = [
        ExecutionTask(task_id=f"task_{i}", func=lambda: task_func(0.01))
        for i in range(10)
    ]

    parallel = ParallelExecutor(max_workers=4)
    start = time.time()
    parallel.execute(tasks)
    par_time_10 = time.time() - start
    parallel.shutdown()

    speedup_10 = seq_time_10 / par_time_10 if par_time_10 > 0 else 0
    status = "pass" if speedup_10 > 1.5 else "fail"
    reporter.add_result(
        "Execution (10)",
        "Parallel (10 tasks)",
        par_time_10,
        status=status,
        details={"speedup": speedup_10, "target_speedup": "> 1.5x"},
    )
    print(
        f"    {'âœ“' if status == 'pass' else 'âœ—'} Parallel (10 tasks): {par_time_10:.4f}s (Speedup: {speedup_10:.2f}x)"
    )

    # æµ‹è¯• 3: 100 ä¸ªä»»åŠ¡ä¸²è¡Œ
    print("  æµ‹è¯• 100 ä¸ªä»»åŠ¡ä¸²è¡Œ...")
    tasks = [
        ExecutionTask(task_id=f"task_{i}", func=lambda: task_func(0.05))
        for i in range(100)
    ]

    sequential = SequentialExecutor()
    start = time.time()
    sequential.execute(tasks)
    seq_time_100 = time.time() - start
    reporter.add_result("Execution (100)", "Sequential (100 tasks)", seq_time_100)
    print(f"    âœ“ Sequential (100 tasks): {seq_time_100:.4f}s")

    # æµ‹è¯• 4: 100 ä¸ªä»»åŠ¡å¹¶è¡Œ
    print("  æµ‹è¯• 100 ä¸ªä»»åŠ¡å¹¶è¡Œ (4 workers)...")
    tasks = [
        ExecutionTask(task_id=f"task_{i}", func=lambda: task_func(0.05))
        for i in range(100)
    ]

    parallel = ParallelExecutor(max_workers=4)
    start = time.time()
    parallel.execute(tasks)
    par_time_100 = time.time() - start
    parallel.shutdown()

    speedup_100 = seq_time_100 / par_time_100 if par_time_100 > 0 else 0
    status = "pass" if speedup_100 > 2.0 and par_time_100 < 30 else "fail"
    reporter.add_result(
        "Execution (100)",
        "Parallel (100 tasks, 4 workers)",
        par_time_100,
        status=status,
        details={
            "speedup": speedup_100,
            "target_time": "< 30s",
            "target_speedup": "> 2x",
        },
    )
    print(
        f"    {'âœ“' if status == 'pass' else 'âœ—'} Parallel (100 tasks): {par_time_100:.4f}s (Speedup: {speedup_100:.2f}x)"
    )


def run_report_generation_benchmark(reporter: BenchmarkReporter):
    """æŠ¥å‘Šç”Ÿæˆæ€§èƒ½æµ‹è¯• - å¤§è§„æ¨¡"""
    print("\nğŸ“Š Running Report Generation Benchmarks...")

    from ptest.reports.enhanced_generator import (
        EnhancedReportGenerator,
        ReportData,
        TestResult,
    )

    import tempfile

    with tempfile.TemporaryDirectory() as tmpdir:
        generator = EnhancedReportGenerator(output_dir=tmpdir)

        # æµ‹è¯• 1: 100 ä¸ªç»“æœ
        print("  æµ‹è¯• 100 ä¸ªç»“æœæŠ¥å‘Š...")
        results = [
            TestResult(
                case_id=f"test_{i}",
                status="passed" if i % 3 != 0 else "failed",
                duration=0.1,
            )
            for i in range(100)
        ]

        data = ReportData(
            title="Benchmark Report (100)",
            total=100,
            passed=67,
            failed=33,
            duration=10.0,
            results=results,
        )

        start = time.time()
        report_path = generator.generate(data)
        elapsed = time.time() - start
        reporter.add_result(
            "Report Generation (100)", "Generate Report (100 results)", elapsed
        )
        print(
            f"    âœ“ Generate report (100 results): {elapsed:.4f}s -> {report_path.name}"
        )

        # æµ‹è¯• 2: 1,000 ä¸ªç»“æœ
        print("  æµ‹è¯• 1,000 ä¸ªç»“æœæŠ¥å‘Š...")
        results = [
            TestResult(
                case_id=f"test_{i}",
                status="passed" if i % 5 != 0 else "failed",
                duration=0.1,
            )
            for i in range(1000)
        ]

        data = ReportData(
            title="Benchmark Report (1K)",
            total=1000,
            passed=800,
            failed=200,
            duration=100.0,
            results=results,
        )

        result, elapsed, peak_mb = measure_memory_and_time(generator.generate, data)
        status = "pass" if elapsed < 2.0 and peak_mb < 500 else "fail"
        reporter.add_result(
            "Report Generation (1K)",
            "Generate Report (1K results)",
            elapsed,
            status=status,
            details={
                "memory_mb": peak_mb,
                "target_time": "< 2s",
                "target_memory": "< 500MB",
            },
        )
        print(
            f"    {'âœ“' if status == 'pass' else 'âœ—'} Generate report (1,000 results): {elapsed:.4f}s, Memory: {peak_mb:.2f}MB"
        )


def run_isolation_engine_benchmark(reporter: BenchmarkReporter):
    """éš”ç¦»å¼•æ“æ€§èƒ½æµ‹è¯•"""
    print("\nğŸ”’ Running Isolation Engine Benchmarks...")

    from ptest.isolation import IsolationManager

    manager = IsolationManager()

    # è¿è¡Œå¼•æ“åŸºå‡†æµ‹è¯•
    print("  è¿è¡Œå¼•æ“åŸºå‡†æµ‹è¯• (creation, activation, command_exec)...")
    results = manager.benchmark_engines(
        test_scenarios=["creation", "activation", "command_exec"]
    )

    for level, data in results.items():
        print(f"\n  Engine: {level}")
        benchmarks = data.get("benchmarks", {})

        if "creation_time" in benchmarks:
            target = {"basic": 1.0, "virtualenv": 5.0, "docker": 10.0}.get(level, 5.0)
            status = "pass" if benchmarks["creation_time"] < target else "fail"
            reporter.add_result(
                f"Isolation ({level})",
                "Environment Creation",
                benchmarks["creation_time"],
                status=status,
                details={"target": f"< {target}s"},
            )
            print(
                f"    {'âœ“' if status == 'pass' else 'âœ—'} Creation: {benchmarks['creation_time']:.4f}s (target: < {target}s)"
            )

        if "activation_time" in benchmarks:
            reporter.add_result(
                f"Isolation ({level})",
                "Environment Activation",
                benchmarks["activation_time"],
            )
            print(f"    âœ“ Activation: {benchmarks['activation_time']:.4f}s")

        if "command_exec_time" in benchmarks:
            reporter.add_result(
                f"Isolation ({level})",
                "Command Execution",
                benchmarks["command_exec_time"],
            )
            print(f"    âœ“ Command exec: {benchmarks['command_exec_time']:.4f}s")


def run_all_benchmarks():
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    print("=" * 70)
    print("ğŸš€ ptestx Large-Scale Performance Benchmark Suite")
    print("=" * 70)
    print("\næµ‹è¯•çº§åˆ«:")
    print("  â€¢ åŸºå‡†çº§: 1,000 æ•°æ® / 100 ç”¨ä¾‹")
    print("  â€¢ æ ‡å‡†çº§: 100,000 æ•°æ® / 1,000 ç”¨ä¾‹")
    print("  â€¢ å‹åŠ›çº§: 1,000,000 æ•°æ® / å¤§è§„æ¨¡å¹¶å‘")
    print()

    reporter = BenchmarkReporter()

    try:
        # è¿è¡Œå„ç±»åŸºå‡†æµ‹è¯•
        run_data_generation_benchmark(reporter)
        run_suite_management_benchmark(reporter)
        run_parallel_execution_benchmark(reporter)
        run_report_generation_benchmark(reporter)
        run_isolation_engine_benchmark(reporter)

        # ç”ŸæˆæŠ¥å‘Š
        print("\n" + "=" * 70)
        print("ğŸ“Š Generating Reports...")
        print("=" * 70)

        json_path = reporter.save_json()
        md_path = reporter.save_markdown()

        # æ‰“å°æ‘˜è¦
        print("\nğŸ“ˆ Summary:")
        report = reporter.generate_report()
        total_tests = 0
        total_passed = 0
        total_failed = 0

        for category, stats in report["summary"].items():
            total_tests += stats["count"]
            total_passed += stats["passed"]
            total_failed += stats["failed"]
            print(f"  {category}:")
            print(
                f"    - Tests: {stats['count']} (âœ“ {stats['passed']}, âœ— {stats['failed']})"
            )
            if stats["count"] > 0:
                print(f"    - Total: {stats['total']:.4f}s")
                print(f"    - Average: {stats['avg']:.4f}s")

        print(f"\n{'=' * 70}")
        print(f"âœ… All benchmarks completed!")
        print(f"   Total: {total_tests} tests (âœ“ {total_passed}, âœ— {total_failed})")
        print(f"ğŸ“ Results saved to: {reporter.output_dir}")
        print(f"ğŸ“Š JSON: {json_path.name}")
        print(f"ğŸ“ Markdown: {md_path.name}")
        print("=" * 70)

        return 0 if total_failed == 0 else 1

    except Exception as e:
        print(f"\nâŒ Benchmark failed: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(run_all_benchmarks())
