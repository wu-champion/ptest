# ptest API ä½¿ç”¨ç¤ºä¾‹

## ğŸ“š ç¤ºä¾‹æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº† ptest Python API çš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ï¼Œæ¶µç›–ä»åŸºç¡€æ“ä½œåˆ°é«˜çº§åŠŸèƒ½çš„å®Œæ•´åœºæ™¯ã€‚

## ğŸš€ åŸºç¡€ç¤ºä¾‹

### ç¤ºä¾‹1: ç®€å•çš„APIæµ‹è¯•

```python
from ptest import TestFramework

# åˆ›å»ºæ¡†æ¶å®ä¾‹
framework = TestFramework()

# åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
env = framework.create_environment("./api_test", isolation="virtualenv")

# æ·»åŠ APIæµ‹è¯•ç”¨ä¾‹
env.add_case("jsonplaceholder_test", {
    "type": "api",
    "method": "GET",
    "url": "https://jsonplaceholder.typicode.com/users/1",
    "expected_status": 200,
    "assertions": [
        {"type": "json_path", "path": "$.name", "operator": "exists"},
        {"type": "json_path", "path": "$.email", "operator": "contains", "value": "@"}
    ]
})

# è¿è¡Œæµ‹è¯•
result = env.run_case("jsonplaceholder_test")

# æ£€æŸ¥ç»“æœ
if result.is_passed():
    print("âœ… APIæµ‹è¯•é€šè¿‡")
    print(f"å“åº”æ—¶é—´: {result.get_duration():.2f}ç§’")
else:
    print("âŒ APIæµ‹è¯•å¤±è´¥")
    print(f"é”™è¯¯ä¿¡æ¯: {result.get_error()}")

# ç”ŸæˆæŠ¥å‘Š
report = env.generate_report("html")
print(f"ğŸ“Š æŠ¥å‘Šå·²ç”Ÿæˆ: {report}")

# æ¸…ç†èµ„æº
framework.cleanup()
```

### ç¤ºä¾‹2: æ•°æ®åº“é›†æˆæµ‹è¯•

```python
from ptest import TestFramework

framework = TestFramework()
env = framework.create_environment("./db_test", isolation="virtualenv")

# å®‰è£…æ•°æ®åº“è¿æ¥åŒ…
env.install_package("mysql-connector-python==8.0.0")

# æ·»åŠ MySQLæ•°æ®åº“å¯¹è±¡
mysql = env.add_object("mysql", "test_db", 
                         version="8.0",
                         port=3306,
                         user="test_user",
                         password="test_pass",
                         database="test_db")

# å¯åŠ¨æ•°æ®åº“
mysql.start()

# ç­‰å¾…æ•°æ®åº“å¯åŠ¨
import time
time.sleep(5)

# æ·»åŠ æ•°æ®åº“æµ‹è¯•ç”¨ä¾‹
env.add_case("mysql_connection_test", {
    "type": "database",
    "object": "test_db",
    "setup": [
        "CREATE TABLE test_users (id INT PRIMARY KEY, name VARCHAR(50), email VARCHAR(100))"
    ],
    "tests": [
        {
            "query": "INSERT INTO test_users (id, name, email) VALUES (1, 'John Doe', 'john@example.com')",
            "expected_affected_rows": 1
        },
        {
            "query": "SELECT * FROM test_users WHERE id = 1",
            "expected_results": [{"id": 1, "name": "John Doe", "email": "john@example.com"}]
        }
    ],
    "cleanup": [
        "DROP TABLE test_users"
    ]
})

# è¿è¡Œæµ‹è¯•
result = env.run_case("mysql_connection_test")
print(f"æ•°æ®åº“æµ‹è¯•ç»“æœ: {'é€šè¿‡' if result.is_passed() else 'å¤±è´¥'}")

# åœæ­¢æ•°æ®åº“
mysql.stop()

framework.cleanup()
```

## ğŸ”§ ä¸­çº§ç¤ºä¾‹

### ç¤ºä¾‹3: å¤šç¯å¢ƒå¹¶å‘æµ‹è¯•

```python
from ptest import TestFramework
from concurrent.futures import ThreadPoolExecutor, as_completed
import tempfile
import shutil

def run_test_in_environment(env_name, test_config):
    """åœ¨ç‹¬ç«‹ç¯å¢ƒä¸­è¿è¡Œæµ‹è¯•"""
    framework = TestFramework()
    
    try:
        # åˆ›å»ºä¸´æ—¶ç¯å¢ƒ
        temp_dir = tempfile.mkdtemp(prefix=f"ptest_{env_name}_")
        env = framework.create_environment(temp_dir, isolation="virtualenv")
        
        # å®‰è£…ä¾èµ–åŒ…
        if "packages" in test_config:
            for package in test_config["packages"]:
                env.install_package(package)
        
        # æ·»åŠ æµ‹è¯•ç”¨ä¾‹
        env.add_case(f"{env_name}_test", test_config["test_case"])
        
        # è¿è¡Œæµ‹è¯•
        result = env.run_case(f"{env_name}_test")
        
        return {
            "env_name": env_name,
            "success": result.is_passed(),
            "duration": result.get_duration(),
            "error": result.get_error() if not result.is_passed() else None
        }
        
    except Exception as e:
        return {
            "env_name": env_name,
            "success": False,
            "duration": 0,
            "error": str(e)
        }
    finally:
        # æ¸…ç†èµ„æº
        if 'framework' in locals():
            framework.cleanup()
        if 'temp_dir' in locals() and os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

# å®šä¹‰æµ‹è¯•é…ç½®
test_configs = {
    "api_test": {
        "packages": ["requests==2.28.0"],
        "test_case": {
            "type": "api",
            "method": "GET",
            "url": "https://jsonplaceholder.typicode.com/users/1",
            "expected_status": 200
        }
    },
    "math_test": {
        "packages": ["numpy==1.24.0"],
        "test_case": {
            "type": "python",
            "code": "import numpy as np; result = np.array([1, 2, 3]).sum(); assert result == 6"
        }
    },
    "web_test": {
        "packages": ["flask==2.2.0"],
        "test_case": {
            "type": "web",
            "setup": "from flask import Flask; app = Flask(__name__)",
            "test": "with app.test_client() as client: response = client.get('/'); assert response.status_code == 200"
        }
    }
}

# å¹¶å‘è¿è¡Œæµ‹è¯•
with ThreadPoolExecutor(max_workers=3) as executor:
    # æäº¤æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    futures = {
        executor.submit(run_test_in_environment, env_name, config): env_name
        for env_name, config in test_configs.items()
    }
    
    # æ”¶é›†ç»“æœ
    results = []
    for future in as_completed(futures):
        env_name = futures[future]
        try:
            result = future.result()
            results.append(result)
            status = "âœ…" if result["success"] else "âŒ"
            print(f"{status} {env_name}: {result['duration']:.2f}s")
            if not result["success"]:
                print(f"   é”™è¯¯: {result['error']}")
        except Exception as e:
            print(f"âŒ {env_name}: æ‰§è¡Œå¤±è´¥ - {e}")

# ç»Ÿè®¡ç»“æœ
success_count = sum(1 for r in results if r["success"])
total_count = len(results)
avg_duration = sum(r["duration"] for r in results) / total_count

print(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
print(f"   æ€»æ•°: {total_count}")
print(f"   æˆåŠŸ: {success_count}")
print(f"   å¤±è´¥: {total_count - success_count}")
print(f"   æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
print(f"   å¹³å‡è€—æ—¶: {avg_duration:.2f}s")
```

### ç¤ºä¾‹4: å¾®æœåŠ¡é›†æˆæµ‹è¯•

```python
from ptest import TestFramework
import time

framework = TestFramework()
env = framework.create_environment("./microservice_test", isolation="docker")

# å¾®æœåŠ¡é…ç½®
services = {
    "user_service": {
        "type": "web",
        "url": "http://localhost:8001",
        "port": 8001,
        "health_check": "/health"
    },
    "order_service": {
        "type": "web", 
        "url": "http://localhost:8002",
        "port": 8002,
        "health_check": "/health"
    },
    "payment_service": {
        "type": "web",
        "url": "http://localhost:8003", 
        "port": 8003,
        "health_check": "/health"
    }
}

# å¯åŠ¨æ‰€æœ‰æœåŠ¡
service_objects = {}
for service_name, config in services.items():
    service_obj = env.add_object("web", service_name, **config)
    service_obj.start()
    service_objects[service_name] = service_obj

# ç­‰å¾…æœåŠ¡å¯åŠ¨
print("â³ ç­‰å¾…æœåŠ¡å¯åŠ¨...")
time.sleep(10)

# å¥åº·æ£€æŸ¥
all_healthy = True
for service_name, service_obj in service_objects.items():
    is_healthy = service_obj.health_check()
    status = "âœ…" if is_healthy else "âŒ"
    print(f"{status} {service_name}: {'å¥åº·' if is_healthy else 'ä¸å¥åº·'}")
    if not is_healthy:
        all_healthy = False

if all_healthy:
    # åˆ›å»ºé›†æˆæµ‹è¯•ç”¨ä¾‹
    env.add_case("user_order_payment_flow", {
        "type": "integration",
        "description": "ç”¨æˆ·-è®¢å•-æ”¯ä»˜æµç¨‹æµ‹è¯•",
        "steps": [
            {
                "service": "user_service",
                "method": "POST",
                "path": "/users",
                "data": {"name": "John Doe", "email": "john@example.com"},
                "expected_status": 201,
                "extract": {"user_id": "$.id"}
            },
            {
                "service": "order_service", 
                "method": "POST",
                "path": "/orders",
                "data": {"user_id": "${user_id}", "items": [{"product_id": 1, "quantity": 2}]},
                "expected_status": 201,
                "extract": {"order_id": "$.id"}
            },
            {
                "service": "payment_service",
                "method": "POST", 
                "path": "/payments",
                "data": {"order_id": "${order_id}", "amount": 100.0, "method": "credit_card"},
                "expected_status": 200
            }
        ],
        "assertions": [
            {"step": 1, "status_code": 201},
            {"step": 2, "status_code": 201},
            {"step": 3, "status_code": 200}
        ]
    })
    
    # è¿è¡Œé›†æˆæµ‹è¯•
    result = env.run_case("user_order_payment_flow")
    
    if result.is_passed():
        print("âœ… å¾®æœåŠ¡é›†æˆæµ‹è¯•é€šè¿‡")
    else:
        print("âŒ å¾®æœåŠ¡é›†æˆæµ‹è¯•å¤±è´¥")
        print(f"é”™è¯¯: {result.get_error()}")
    
    # ç”Ÿæˆè¯¦ç»†çš„é›†æˆæµ‹è¯•æŠ¥å‘Š
    report = env.generate_report("html")
    print(f"ğŸ“Š é›†æˆæµ‹è¯•æŠ¥å‘Š: {report}")
else:
    print("âŒ æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡é›†æˆæµ‹è¯•")

# åœæ­¢æ‰€æœ‰æœåŠ¡
for service_name, service_obj in service_objects.items():
    service_obj.stop()
    print(f"ğŸ›‘ å·²åœæ­¢ {service_name}")

framework.cleanup()
```

## ğŸš€ é«˜çº§ç¤ºä¾‹

### ç¤ºä¾‹5: æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
from ptest import TestFramework
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

class PerformanceBenchmark:
    def __init__(self, framework, env):
        self.framework = framework
        self.env = env
        self.results = []
    
    def run_load_test(self, test_case, concurrent_users=10, duration=30):
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹è´Ÿè½½æµ‹è¯•: {concurrent_users} å¹¶å‘ç”¨æˆ·, {duration}ç§’")
        
        def single_user_test():
            """å•ä¸ªç”¨æˆ·æµ‹è¯•"""
            start_time = time.time()
            result = self.env.run_case("load_test_case")
            end_time = time.time()
            
            return {
                "success": result.is_passed(),
                "duration": end_time - start_time,
                "timestamp": start_time
            }
        
        # æ·»åŠ è´Ÿè½½æµ‹è¯•ç”¨ä¾‹
        self.env.add_case("load_test_case", test_case)
        
        # æ‰§è¡Œè´Ÿè½½æµ‹è¯•
        start_time = time.time()
        end_time = start_time + duration
        
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = []
            
            while time.time() < end_time:
                # æäº¤æµ‹è¯•ä»»åŠ¡
                future = executor.submit(single_user_test)
                futures.append(future)
                
                # æ§åˆ¶æäº¤é¢‘ç‡
                time.sleep(0.1)
            
            # æ”¶é›†ç»“æœ
            for future in futures:
                try:
                    result = future.result()
                    self.results.append(result)
                except Exception as e:
                    print(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        
        return self.analyze_results()
    
    def analyze_results(self):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        if not self.results:
            return {"error": "æ²¡æœ‰æµ‹è¯•ç»“æœ"}
        
        successful_results = [r for r in self.results if r["success"]]
        durations = [r["duration"] for r in successful_results]
        
        if not durations:
            return {"error": "æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ"}
        
        analysis = {
            "total_requests": len(self.results),
            "successful_requests": len(successful_results),
            "success_rate": len(successful_results) / len(self.results) * 100,
            "avg_response_time": statistics.mean(durations),
            "min_response_time": min(durations),
            "max_response_time": max(durations),
            "median_response_time": statistics.median(durations),
            "p95_response_time": self.percentile(durations, 95),
            "p99_response_time": self.percentile(durations, 99),
            "requests_per_second": len(successful_results) / (max(r["timestamp"] for r in self.results) - min(r["timestamp"] for r in self.results))
        }
        
        return analysis
    
    @staticmethod
    def percentile(data, percentile):
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_data = sorted(data)
        index = (percentile / 100) * len(sorted_data)
        if index.is_integer():
            return sorted_data[int(index)]
        else:
            lower = sorted_data[int(index)]
            upper = sorted_data[int(index) + 1]
            return lower + (upper - lower) * (index - int(index))

# ä½¿ç”¨ç¤ºä¾‹
framework = TestFramework()
env = framework.create_environment("./performance_test", isolation="virtualenv")

# å®‰è£…æ€§èƒ½æµ‹è¯•ä¾èµ–
env.install_package("requests==2.28.0")
env.install_package("locust==2.15.0")

# å®šä¹‰APIæµ‹è¯•ç”¨ä¾‹
api_test_case = {
    "type": "api",
    "method": "GET",
    "url": "https://jsonplaceholder.typicode.com/users",
    "expected_status": 200,
    "timeout": 10
}

# è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
benchmark = PerformanceBenchmark(framework, env)
results = benchmark.run_load_test(api_test_case, concurrent_users=5, duration=15)

# è¾“å‡ºç»“æœ
print("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
print(f"   æ€»è¯·æ±‚æ•°: {results['total_requests']}")
print(f"   æˆåŠŸè¯·æ±‚æ•°: {results['successful_requests']}")
print(f"   æˆåŠŸç‡: {results['success_rate']:.2f}%")
print(f"   å¹³å‡å“åº”æ—¶é—´: {results['avg_response_time']:.3f}s")
print(f"   æœ€å°å“åº”æ—¶é—´: {results['min_response_time']:.3f}s")
print(f"   æœ€å¤§å“åº”æ—¶é—´: {results['max_response_time']:.3f}s")
print(f"   95%å“åº”æ—¶é—´: {results['p95_response_time']:.3f}s")
print(f"   99%å“åº”æ—¶é—´: {results['p99_response_time']:.3f}s")
print(f"   RPS: {results['requests_per_second']:.2f}")

framework.cleanup()
```

### ç¤ºä¾‹6: è‡ªå®šä¹‰æµ‹è¯•æ¡†æ¶æ‰©å±•

```python
from ptest import TestFramework, TestEnvironment, TestResult
from ptest.isolation import IsolationEngine, IsolatedEnvironment
import json
import time

class CustomTestEnvironment(TestEnvironment):
    """è‡ªå®šä¹‰æµ‹è¯•ç¯å¢ƒï¼Œæ·»åŠ é¢å¤–åŠŸèƒ½"""
    
    def __init__(self, path, isolation="basic", framework=None):
        super().__init__(path, isolation, framework)
        self.custom_data = {}
        self.start_time = None
        self.end_time = None
    
    def start_timing(self):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.time()
    
    def end_timing(self):
        """ç»“æŸè®¡æ—¶"""
        self.end_time = time.time()
    
    def get_duration(self):
        """è·å–æ‰§è¡Œæ—¶é•¿"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0
    
    def set_custom_data(self, key, value):
        """è®¾ç½®è‡ªå®šä¹‰æ•°æ®"""
        self.custom_data[key] = value
    
    def get_custom_data(self, key):
        """è·å–è‡ªå®šä¹‰æ•°æ®"""
        return self.custom_data.get(key)
    
    def export_metrics(self):
        """å¯¼å‡ºæµ‹è¯•æŒ‡æ ‡"""
        return {
            "duration": self.get_duration(),
            "custom_data": self.custom_data,
            "environment_info": self.get_status()
        }

class CustomTestFramework(TestFramework):
    """è‡ªå®šä¹‰æµ‹è¯•æ¡†æ¶ï¼Œæ‰©å±•åŸºç¡€åŠŸèƒ½"""
    
    def __init__(self, config=None):
        super().__init__(config)
        self.test_metrics = []
        self.global_hooks = {
            "before_test": [],
            "after_test": [],
            "before_suite": [],
            "after_suite": []
        }
    
    def add_hook(self, event, callback):
        """æ·»åŠ é’©å­å‡½æ•°"""
        if event in self.global_hooks:
            self.global_hooks[event].append(callback)
    
    def run_hooks(self, event, *args, **kwargs):
        """æ‰§è¡Œé’©å­å‡½æ•°"""
        for hook in self.global_hooks.get(event, []):
            try:
                hook(*args, **kwargs)
            except Exception as e:
                print(f"é’©å­æ‰§è¡Œå¤±è´¥ ({event}): {e}")
    
    def create_custom_environment(self, path, isolation="basic"):
        """åˆ›å»ºè‡ªå®šä¹‰æµ‹è¯•ç¯å¢ƒ"""
        env = CustomTestEnvironment(path, isolation, self)
        self.environments[path] = env
        return env
    
    def run_test_suite(self, test_configs):
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        self.run_hooks("before_suite")
        
        suite_results = []
        suite_start_time = time.time()
        
        for test_config in test_configs:
            self.run_hooks("before_test", test_config)
            
            try:
                # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
                env = self.create_custom_environment(
                    f"./test_{test_config['name']}", 
                    test_config.get("isolation", "basic")
                )
                
                # å¼€å§‹è®¡æ—¶
                env.start_timing()
                
                # è®¾ç½®è‡ªå®šä¹‰æ•°æ®
                env.set_custom_data("test_name", test_config["name"])
                env.set_custom_data("test_config", test_config)
                
                # æ‰§è¡Œæµ‹è¯•
                if test_config["type"] == "api":
                    result = self._run_api_test(env, test_config)
                elif test_config["type"] == "database":
                    result = self._run_database_test(env, test_config)
                else:
                    result = self._run_generic_test(env, test_config)
                
                # ç»“æŸè®¡æ—¶
                env.end_timing()
                
                # æ”¶é›†æŒ‡æ ‡
                metrics = env.export_metrics()
                self.test_metrics.append(metrics)
                
                suite_results.append({
                    "test_name": test_config["name"],
                    "success": result.is_passed(),
                    "duration": metrics["duration"],
                    "metrics": metrics
                })
                
            except Exception as e:
                suite_results.append({
                    "test_name": test_config["name"],
                    "success": False,
                    "duration": 0,
                    "error": str(e)
                })
            
            finally:
                self.run_hooks("after_test", test_config)
        
        suite_end_time = time.time()
        suite_duration = suite_end_time - suite_start_time
        
        self.run_hooks("after_suite")
        
        return {
            "results": suite_results,
            "duration": suite_duration,
            "metrics": {
                "total_tests": len(test_configs),
                "passed_tests": sum(1 for r in suite_results if r["success"]),
                "failed_tests": sum(1 for r in suite_results if not r["success"]),
                "success_rate": sum(1 for r in suite_results if r["success"]) / len(suite_results) * 100
            }
        }
    
    def _run_api_test(self, env, config):
        """è¿è¡ŒAPIæµ‹è¯•"""
        env.add_case("api_test", config["test_case"])
        return env.run_case("api_test")
    
    def _run_database_test(self, env, config):
        """è¿è¡Œæ•°æ®åº“æµ‹è¯•"""
        env.add_case("db_test", config["test_case"])
        return env.run_case("db_test")
    
    def _run_generic_test(self, env, config):
        """è¿è¡Œé€šç”¨æµ‹è¯•"""
        env.add_case("generic_test", config["test_case"])
        return env.run_case("generic_test")
    
    def generate_custom_report(self, results, format="json"):
        """ç”Ÿæˆè‡ªå®šä¹‰æŠ¥å‘Š"""
        if format == "json":
            return json.dumps(results, indent=2)
        elif format == "html":
            return self._generate_html_report(results)
        else:
            return str(results)
    
    def _generate_html_report(self, results):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>ptest è‡ªå®šä¹‰æŠ¥å‘Š</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .header { background: #f0f0f0; padding: 20px; border-radius: 5px; }
                .test-result { margin: 10px 0; padding: 10px; border-left: 4px solid #ccc; }
                .success { border-left-color: #4CAF50; }
                .failure { border-left-color: #f44336; }
                .metrics { background: #f9f9f9; padding: 10px; margin: 10px 0; border-radius: 3px; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ptest æµ‹è¯•æŠ¥å‘Š</h1>
                <p>æ€»æµ‹è¯•æ•°: {total_tests}</p>
                <p>é€šè¿‡æ•°: {passed_tests}</p>
                <p>å¤±è´¥æ•°: {failed_tests}</p>
                <p>æˆåŠŸç‡: {success_rate:.1f}%</p>
                <p>æ€»è€—æ—¶: {duration:.2f}ç§’</p>
            </div>
        """.format(**results["metrics"])
        
        for result in results["results"]:
            css_class = "success" if result["success"] else "failure"
            html += f"""
            <div class="test-result {css_class}">
                <h3>{result['test_name']}</h3>
                <p>çŠ¶æ€: {'é€šè¿‡' if result['success'] else 'å¤±è´¥'}</p>
                <p>è€—æ—¶: {result['duration']:.3f}ç§’</p>
            </div>
            """
        
        html += """
        </body>
        </html>
        """
        return html

# ä½¿ç”¨ç¤ºä¾‹
def before_test_hook(test_config):
    print(f"ğŸš€ å¼€å§‹æµ‹è¯•: {test_config['name']}")

def after_test_hook(test_config):
    print(f"âœ… æµ‹è¯•å®Œæˆ: {test_config['name']}")

# åˆ›å»ºè‡ªå®šä¹‰æ¡†æ¶
framework = CustomTestFramework()

# æ·»åŠ é’©å­
framework.add_hook("before_test", before_test_hook)
framework.add_hook("after_test", after_test_hook)

# å®šä¹‰æµ‹è¯•é…ç½®
test_configs = [
    {
        "name": "api_test_1",
        "type": "api",
        "isolation": "virtualenv",
        "test_case": {
            "type": "api",
            "method": "GET",
            "url": "https://jsonplaceholder.typicode.com/users/1",
            "expected_status": 200
        }
    },
    {
        "name": "api_test_2", 
        "type": "api",
        "isolation": "basic",
        "test_case": {
            "type": "api",
            "method": "GET",
            "url": "https://jsonplaceholder.typicode.com/posts/1",
            "expected_status": 200
        }
    }
]

# è¿è¡Œæµ‹è¯•å¥—ä»¶
results = framework.run_test_suite(test_configs)

# ç”Ÿæˆè‡ªå®šä¹‰æŠ¥å‘Š
json_report = framework.generate_custom_report(results, "json")
html_report = framework.generate_custom_report(results, "html")

# ä¿å­˜æŠ¥å‘Š
with open("custom_report.json", "w") as f:
    f.write(json_report)

with open("custom_report.html", "w") as f:
    f.write(html_report)

print("ğŸ“Š è‡ªå®šä¹‰æŠ¥å‘Šå·²ç”Ÿæˆ:")
print(f"   JSON: custom_report.json")
print(f"   HTML: custom_report.html")

framework.cleanup()
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. èµ„æºç®¡ç†
```python
# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ¸…ç†
with TestFramework() as framework:
    env = framework.create_environment("./test")
    # æµ‹è¯•ä»£ç 
# è‡ªåŠ¨æ¸…ç†
```

### 2. é”™è¯¯å¤„ç†
```python
try:
    result = env.run_case("test")
    if result.is_passed():
        print("æµ‹è¯•é€šè¿‡")
    else:
        print(f"æµ‹è¯•å¤±è´¥: {result.get_error()}")
except Exception as e:
    print(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
```

### 3. é…ç½®ç®¡ç†
```python
# ä½¿ç”¨é…ç½®æ–‡ä»¶
import json

with open("test_config.json", "r") as f:
    config = json.load(f)

framework = TestFramework(config)
```

### 4. æ—¥å¿—è®°å½•
```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ptest")

# åœ¨æµ‹è¯•ä¸­ä½¿ç”¨æ—¥å¿—
logger.info("å¼€å§‹æµ‹è¯•æ‰§è¡Œ")
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [Python API å‚è€ƒ](python-api.md)
- [ç¯å¢ƒç®¡ç†æŒ‡å—](../guides/environment-management.md)
- [æµ‹è¯•ç”¨ä¾‹ç¼–å†™](../guides/test-case-writing.md)
- [å¯¹è±¡ç®¡ç†æŒ‡å—](../guides/object-management.md)

---

**ç¤ºä¾‹ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2026-01-25  
**ç»´æŠ¤è€…**: cp